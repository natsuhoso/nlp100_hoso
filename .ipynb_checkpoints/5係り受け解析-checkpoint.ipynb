{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "    def __str__(self):\n",
    "        return self.surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentences = []\n",
    "sentence = []\n",
    "for line in open('data/neko.txt.cabocha'):\n",
    "    x = re.search('(?P<surface>[^\\t]*?)\\t(?P<pos>[^,]*),(?P<pos1>[^,]*),[^,]*,[^,]*,[^,]*,[^,]*,(?P<base>[^,]*).*',line)\n",
    "    if x:\n",
    "        morph = Morph(x.group('surface'), x.group('base'), x.group('pos'), x.group('pos1'))\n",
    "        sentence.append(morph)\n",
    "    elif line == 'EOS\\n' and not sentence == []:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['一'], ['\\u3000', '吾輩', 'は', '猫', 'で', 'ある', '。'], ['名前', 'は', 'まだ', '無い', '。'], ['\\u3000', 'どこ', 'で', '生れ', 'た', 'か', 'とんと', '見当', 'が', 'つか', 'ぬ', '。'], ['何', 'でも', '薄暗い', 'じめじめ', 'し', 'た', '所', 'で', 'ニャーニャー', '泣い', 'て', 'いた事', 'だけ', 'は', '記憶', 'し', 'て', 'いる', '。'], ['吾輩', 'は', 'ここ', 'で', '始め', 'て', '人間', 'という', 'もの', 'を', '見', 'た', '。'], ['しかも', 'あと', 'で', '聞く', 'と', 'それ', 'は', '書生', 'という', '人間', '中', 'で', '一番', '獰悪', 'な', '種族', 'で', 'あっ', 'た', 'そう', 'だ', '。'], ['この', '書生', 'という', 'の', 'は', '時々', '我々', 'を', '捕え', 'て', '煮', 'て', '食う', 'という', '話', 'で', 'ある', '。'], ['しかし', 'その', '当時', 'は', '何', 'という', '考', 'も', 'なかっ', 'た', 'から', '別段', '恐し', 'いとも', '思わ', 'なかっ', 'た', '。'], ['ただ', '彼', 'の', '掌', 'に', '載せ', 'られ', 'て', 'スー', 'と', '持ち上げ', 'られ', 'た', '時', '何だか', 'フワフワ', 'し', 'た', '感じ', 'が', 'あっ', 'た', 'ばかり', 'で', 'ある', '。']]\n"
     ]
    }
   ],
   "source": [
    "print([[str(y) for y in x] for x in sentences[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['名前', 'は', 'まだ', '無い', '。']\n"
     ]
    }
   ],
   "source": [
    "print([str(x) for x in sentences[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, morphs, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "    def __str__(self):\n",
    "        ret = ''.join([str(x) for x in self.morphs])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentences = []#sentence の list\n",
    "sentence = []#Chunk の list\n",
    "for line in open('data/neko.txt.cabocha'):\n",
    "    x = re.search('(?P<surface>.*?)\\t(?P<pos>[^,]*),(?P<pos1>[^,]*),[^,]*,[^,]*,[^,]*,[^,]*,(?P<base>[^,]*).*',line)\n",
    "    if x:\n",
    "        morph = Morph(x.group('surface'), x.group('base'), x.group('pos'), x.group('pos1'))\n",
    "        sentence[-1].morphs.append(morph)\n",
    "        continue\n",
    "    y = re.search('\\*\\ (?P<number>[0-9]+)\\ (?P<dst>-?[0-9]*)D.*', line)\n",
    "    if y:\n",
    "        dst = y.group('dst')\n",
    "        srcs = [i for i, j in enumerate(sentence) if j.dst == y.group('number')]\n",
    "        chunk = Chunk([], dst, srcs)\n",
    "        sentence.append(chunk)\n",
    "    elif line == 'EOS\\n' and not sentence == []:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['この->書生というのは', '書生というのは->話である。', '時々->捕えて', '我々を->捕えて', '捕えて->煮て', '煮て->食うという', '食うという->話である。', '話である。']\n"
     ]
    }
   ],
   "source": [
    "s = sentences[7]\n",
    "print([str(i) + '->' + str(s[int(i.dst)])  if i.dst != '-1' else str(i) for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file42', 'w'):\n",
    "    pass\n",
    "def st(s):\n",
    "    return str(s).strip('、。「」')\n",
    "with open('data/file42', 'a') as a:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            p = [st(sentence[int(i)]) for i in chunk.srcs] + ['「'+st(chunk)+'」']\n",
    "            p = p + [st(sentence[int(chunk.dst)])] if chunk.dst != '-1' else p\n",
    "            a.write('\\t'.join(p) +  '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「一」\n",
      "「　」\t猫である\n",
      "「吾輩は」\t猫である\n",
      "　\t吾輩は\t「猫である」\n",
      "「名前は」\t無い\n",
      "「まだ」\t無い\n",
      "名前は\tまだ\t「無い」\n",
      "「　どこで」\t生れたか\n",
      "　どこで\t「生れたか」\tつかぬ\n",
      "「とんと」\tつかぬ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/file42', 'r') as r:\n",
    "    print(''.join(r.readlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file43', 'w'):\n",
    "    pass\n",
    "def st(s):\n",
    "    return str(s).strip('、。「」')\n",
    "with open('data/file43', 'a') as a:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            if not [0 for i in chunk.morphs if i.pos == '名詞']:\n",
    "                continue\n",
    "            chun2 = sentence[int(chunk.dst)]\n",
    "            if not [0 for i in chun2.morphs if i.pos == '動詞']:\n",
    "                continue\n",
    "            a.write(st(chunk) + '\\t' + st(chun2) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　どこで\t生れたか\n",
      "見当が\tつかぬ\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "いた事だけは\t記憶している\n",
      "記憶している\t記憶している\n",
      "吾輩は\t見た\n",
      "ここで\t始めて\n",
      "ものを\t見た\n",
      "あとで\t聞くと\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/file43', 'r') as r:\n",
    "    print(''.join(r.readlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#とりあえず使ってみる\n",
    "import networkx as nx\n",
    "import pygraphviz\n",
    "G = nx.Graph()\n",
    "G.add_node(\"a\", label=\"wow\")\n",
    "G.add_node(\"b\", label=\"あわれ\")\n",
    "G.add_edge(\"a\",\"b\")\n",
    "g = nx.nx_agraph.to_agraph(G)\n",
    "g.draw('data/nx_test.pdf' ,format='pdf', prog='dot') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sentences[4]\n",
    "G = nx.Graph()\n",
    "for chunk in s:\n",
    "    G.add_node(chunk, label=str(chunk))\n",
    "for chunk in s[:-1]:\n",
    "    G.add_edge(chunk, s[int(chunk.dst)])\n",
    "g = nx.nx_agraph.to_agraph(G)\n",
    "g.draw(f\"data/file44.pdf\",format='pdf', prog='dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "- 述語に係る助詞を格とする\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "    始める  で\n",
    "    \n",
    "    見る    は を\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "- 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file45', 'w'):\n",
    "    pass\n",
    "with open('data/file45', 'a') as a:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            if chunk.morphs[-1].surface == '、':\n",
    "                chunk.morphs = chunk.morphs[:-1]\n",
    "            verbs = [i.base for i in chunk.morphs if i.pos == '動詞']\n",
    "            if verbs != []:\n",
    "                verb = verbs[0]\n",
    "                particles =  [sentence[i].morphs[-1].base for i in list(map(int,chunk.srcs)) if sentence[i].morphs[-1].pos == '助詞']\n",
    "                if particles != []:\n",
    "                    a.write(verb + '\\t' + ' '.join(particles) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓スペースで区切ったやつを考慮にいれていない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 705 云う\tと\r\n",
      " 467 する\tを\r\n",
      " 343 思う\tと\r\n",
      " 206 ある\tが\r\n",
      " 203 なる\tに\r\n",
      " 195 する\tに\r\n",
      " 182 見る\tて\r\n",
      " 161 する\tと\r\n",
      " 118 する\tが\r\n",
      " 100 見る\tを\r\n",
      "sort: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!sort data/file45 | uniq -c | sort -r | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file45b', 'w'):\n",
    "    pass\n",
    "with open('data/file45b', 'a') as a:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            if chunk.morphs[-1].surface == '、':\n",
    "                chunk.morphs = chunk.morphs[:-1]\n",
    "            verbs = [i.base for i in chunk.morphs if i.pos == '動詞']\n",
    "            if verbs != []:\n",
    "                verb = verbs[0]\n",
    "                particles =  [sentence[i].morphs[-1].base for i in list(map(int,chunk.srcs)) if sentence[i].morphs[-1].pos == '助詞']\n",
    "                for particle in  particles:\n",
    "                    a.write(verb + '\\t' + particle + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1314 する\tを\r\n",
      "1037 云う\tと\r\n",
      " 778 する\tに\r\n",
      " 578 する\tと\r\n",
      " 577 する\tは\r\n",
      " 518 なる\tに\r\n",
      " 518 する\tが\r\n",
      " 502 ある\tが\r\n",
      " 458 する\tて\r\n",
      " 410 思う\tと\r\n",
      "sort: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!sort data/file45b | uniq -c | sort -r | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1314 する\tを\r\n",
      " 778 する\tに\r\n",
      " 578 する\tと\r\n",
      " 577 する\tは\r\n",
      " 518 する\tが\r\n",
      " 458 する\tて\r\n",
      " 228 する\tから\r\n",
      " 220 する\tも\r\n",
      " 216 する\tで\r\n",
      "  58 する\tの\r\n",
      "  55 する\tか\r\n",
      "  44 する\tば\r\n",
      "  38 する\tへ\r\n",
      "  36 する\tまで\r\n",
      "  35 する\tでも\r\n",
      "  21 する\tとして\r\n",
      "  18 する\tながら\r\n",
      "  17 する\tって\r\n",
      "  16 する\tばかり\r\n",
      "  16 する\tさえ\r\n",
      "  15 する\tので\r\n",
      "  15 する\tたり\r\n",
      "  13 する\tくらい\r\n",
      "  11 する\tにおいて\r\n",
      "  11 する\tだって\r\n",
      "  11 する\tじゃ\r\n",
      "  11 する\tや\r\n",
      "   9 する\tほど\r\n",
      "   8 する\tなんか\r\n",
      "   8 する\tとも\r\n",
      "   8 する\tね\r\n",
      "   7 する\tによって\r\n",
      "   7 する\tについて\r\n",
      "   7 する\tより\r\n",
      "   7 する\tのに\r\n",
      "   7 する\tだけ\r\n",
      "   6 する\tをもって\r\n",
      "   5 する\tんで\r\n",
      "   4 する\tども\r\n",
      "   4 する\tこそ\r\n",
      "   4 する\tし\r\n",
      "   3 する\tにあたって\r\n",
      "   3 する\tと共に\r\n",
      "   3 する\tとか\r\n",
      "   3 する\tちゃ\r\n",
      "   3 する\tすら\r\n",
      "   3 する\tよ\r\n",
      "   3 する\tさ\r\n",
      "   2 する\tに対して\r\n",
      "   2 する\tたって\r\n",
      "   2 する\tけれど\r\n",
      "   2 する\tだり\r\n",
      "   2 する\tわ\r\n",
      "   2 する\tぜ\r\n",
      "   1 する\tに従って\r\n",
      "   1 する\tけれども\r\n",
      "   1 する\tものの\r\n",
      "   1 する\tに対し\r\n",
      "   1 する\tなんて\r\n",
      "   1 する\tけども\r\n",
      "   1 する\tやら\r\n",
      "   1 する\tねえ\r\n",
      "   1 する\tにて\r\n",
      "   1 する\tなり\r\n",
      "   1 する\tなど\r\n",
      "   1 する\tつつ\r\n",
      "   1 する\tだに\r\n",
      "   1 する\tずつ\r\n",
      "   1 する\tな\r\n"
     ]
    }
   ],
   "source": [
    "!grep -x -e 'する.*' data/file45b | sort | uniq -c | sort -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 380 見る\tて\r\n",
      " 176 見る\tを\r\n",
      "  60 見る\tは\r\n",
      "  53 見る\tから\r\n",
      "  45 見る\tと\r\n",
      "  39 見る\tが\r\n",
      "  36 見る\tで\r\n",
      "  35 見る\tに\r\n",
      "  21 見る\tも\r\n",
      "   7 見る\tか\r\n",
      "   6 見る\tながら\r\n",
      "   6 見る\tので\r\n",
      "   6 見る\tたり\r\n",
      "   6 見る\tば\r\n",
      "   4 見る\tばかり\r\n",
      "   4 見る\tまで\r\n",
      "   2 見る\tより\r\n",
      "   2 見る\tって\r\n",
      "   2 見る\tじゃ\r\n",
      "   2 見る\tの\r\n",
      "   2 見る\tね\r\n",
      "   1 見る\tに従って\r\n",
      "   1 見る\tによって\r\n",
      "   1 見る\tなんか\r\n",
      "   1 見る\tとして\r\n",
      "   1 見る\tんで\r\n",
      "   1 見る\tなあ\r\n",
      "   1 見る\tだけ\r\n",
      "   1 見る\tすら\r\n",
      "   1 見る\tさえ\r\n",
      "   1 見る\tへ\r\n",
      "   1 見る\tな\r\n",
      "   1 見る\tし\r\n"
     ]
    }
   ],
   "source": [
    "!grep -x -e '見る.*' data/file45b | sort | uniq -c | sort -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  16 与える\tを\r\n",
      "  12 与える\tに\r\n",
      "   8 与える\tは\r\n",
      "   7 与える\tて\r\n",
      "   2 与える\tも\r\n",
      "   2 与える\tと\r\n",
      "   2 与える\tが\r\n",
      "   1 与える\tけれども\r\n",
      "   1 与える\tとして\r\n",
      "   1 与える\tば\r\n",
      "   1 与える\tで\r\n",
      "   1 与える\tか\r\n"
     ]
    }
   ],
   "source": [
    "!grep -x -e '与える.*' data/file45b | sort | uniq -c | sort -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "    始める  で      ここで\n",
    "\n",
    "    見る    は を   吾輩は ものを"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file46', 'w'):\n",
    "    pass\n",
    "with open('data/file46', 'a') as a:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            if chunk.morphs[-1].surface == '、':\n",
    "                chunk.morphs = chunk.morphs[:-1]\n",
    "            verbs = [i.base for i in chunk.morphs if i.pos == '動詞']\n",
    "            if verbs != []:\n",
    "                verb = verbs[0]\n",
    "                units = [sentence[i] for i in list(map(int,chunk.srcs)) if sentence[i].morphs[-1].pos == '助詞']\n",
    "                particles = [i.morphs[-1].base for i in units]\n",
    "                if particles != []:\n",
    "                    a.write(verb + '\\t' + ' '.join(particles) + '\\t' + ' '.join([str(i) for i in units]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる\tで\t　どこで\n",
      "つく\tか が\t生れたか 見当が\n",
      "泣く\tで\t所で\n",
      "する\tて は\t泣いて いた事だけは\n",
      "始める\tで\tここで\n",
      "見る\tは を\t吾輩は ものを\n",
      "聞く\tで\tあとで\n",
      "捕える\tを\t我々を\n",
      "煮る\tて\t捕えて\n",
      "食う\tて\t煮て\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for line in open('data/file46'):\n",
    "    print(line, end='')\n",
    "    i = i + 1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "- 例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "    返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "- コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file47', 'w'):\n",
    "    pass\n",
    "with open('data/file47', 'a') as a:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            if chunk.morphs[-1].surface == '、':\n",
    "                chunk.morphs = chunk.morphs[:-1]\n",
    "            verbs = [i.base for i in chunk.morphs if i.pos == '動詞']\n",
    "            if verbs != []:\n",
    "                verb = verbs[0]\n",
    "                sahen = [sentence[i] for i in list(map(int,chunk.srcs)) \\\n",
    "                         if sentence[i].morphs[-1].base == 'を' \\\n",
    "                         and sentence[i].morphs[-2].pos1 == 'サ変接続' ]\n",
    "                if sahen == []:\n",
    "                    continue\n",
    "                units = [sentence[i] for i in list(map(int,chunk.srcs)) if sentence[i].morphs[-1].pos == '助詞' and not sentence[i] in sahen]\n",
    "                particles = [i.morphs[-1].base for i in units]\n",
    "                if particles != []:\n",
    "                    a.write(str(sahen[0]) + verb + '\\t' + ' '.join(particles) + '\\t' + ' '.join([str(i) for i in units]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  25 返事をする\r\n",
      "  19 挨拶をする\r\n",
      "  11 話をする\r\n",
      "   8 質問をする\r\n",
      "   7 喧嘩をする\r\n",
      "   6 真似をする\r\n",
      "   5 質問をかける\r\n",
      "   5 相談をする\r\n",
      "   5 昼寝をする\r\n",
      "   4 御辞儀をする\r\n"
     ]
    }
   ],
   "source": [
    "!cut -f1 data/file47 | sort | uniq -c | sort -r | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file47b', 'w'):\n",
    "    pass\n",
    "with open('data/file47b', 'a') as a:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            if chunk.morphs[-1].surface == '、':\n",
    "                chunk.morphs = chunk.morphs[:-1]\n",
    "            verbs = [i.base for i in chunk.morphs if i.pos == '動詞']\n",
    "            if verbs != []:\n",
    "                verb = verbs[0]\n",
    "                sahen = [sentence[i] for i in list(map(int,chunk.srcs)) \\\n",
    "                         if sentence[i].morphs[-1].base == 'を' \\\n",
    "                         and sentence[i].morphs[-2].pos1 == 'サ変接続' ]\n",
    "                if sahen == []:\n",
    "                    continue\n",
    "                units = [sentence[i] for i in list(map(int,chunk.srcs)) if sentence[i].morphs[-1].pos == '助詞' and not sentence[i] in sahen]\n",
    "                particles = [i.morphs[-1].base for i in units]\n",
    "                for unit in units:\n",
    "                    a.write(str(sahen[0]) + verb + '\\t' + unit.morphs[-1].base+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  23 返事をする\tと\r\n",
      "  11 挨拶をする\tと\r\n",
      "   9 返事をする\tは\r\n",
      "   4 質問をかける\tは\r\n",
      "   4 質問をかける\tと\r\n",
      "   4 返事をする\tから\r\n",
      "   4 挨拶をする\tから\r\n",
      "   4 返事をする\tに\r\n",
      "   4 欠伸をする\tて\r\n",
      "   4 挨拶をする\tは\r\n"
     ]
    }
   ],
   "source": [
    "!sort data/file47b | uniq -c | sort -r | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 各文節は（表層形の）形態素列で表現する\n",
    "- パスの開始文節から終了文節に至るまで，各文節の表現を\"->\"で連結する\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "- 吾輩は -> 見た\n",
    "- ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "- 人間という -> ものを -> 見た\n",
    "- ものを -> 見た"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            if chunk.morphs[-1].surface in '、。':\n",
    "                chunk.morphs = chunk.morphs[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file48', 'w'):\n",
    "    pass\n",
    "def dst(chunk, sentence):\n",
    "    dest = int(chunk.dst)\n",
    "    if dest == -1:\n",
    "        return str(chunk) + '\\n'\n",
    "    else:\n",
    "        return str(chunk) + ' -> ' + dst(sentence[dest], sentence)\n",
    "with open('data/file48', 'a') as a:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            nouns = [i.base for i in chunk.morphs if i.pos == '名詞']\n",
    "            if nouns != []:\n",
    "                noun = nouns[0]\n",
    "                a.write(dst(chunk, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一\n",
      "吾輩は -> 猫である\n",
      "猫である\n",
      "名前は -> 無い\n",
      "　どこで -> 生れたか -> つかぬ\n",
      "見当が -> つかぬ\n",
      "何でも -> 薄暗い -> 所で -> 泣いて -> 記憶している\n",
      "所で -> 泣いて -> 記憶している\n",
      "ニャーニャー -> 泣いて -> 記憶している\n",
      "いた事だけは -> 記憶している\n",
      "記憶している\n",
      "吾輩は -> 見た\n",
      "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
      "人間という -> ものを -> 見た\n",
      "ものを -> 見た\n",
      "あとで -> 聞くと -> 種族であったそうだ\n",
      "それは -> 種族であったそうだ\n",
      "書生という -> 人間中で -> 種族であったそうだ\n",
      "人間中で -> 種族であったそうだ\n",
      "一番 -> 獰悪な -> 種族であったそうだ\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for line in open('data/file48'):\n",
    "    print(line, end='')\n",
    "    i = i + 1\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 49. 名詞間の係り受けパスの抽出\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を\"->\"で連結して表現する\n",
    "- 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "- 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
    "- 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を\"|\"で連結して表示\n",
    "\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "- Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "- Xは | Yという -> ものを | 見た\n",
    "- Xは | Yを | 見た\n",
    "- Xで -> 始めて -> Y\n",
    "- Xで -> 始めて -> 人間という -> Y\n",
    "- Xという -> Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file49', 'w'):\n",
    "    pass\n",
    "\n",
    "def stri(chunk, nX, nY):\n",
    "    if chunk == nX:\n",
    "        return ''.join(['X' if i == [j for j in chunk.morphs if j.pos == '名詞'][0] else str(i) for i in chunk.morphs])\n",
    "    elif chunk == nY:\n",
    "        return ''.join(['Y' if i == [j for j in chunk.morphs if j.pos == '名詞'][0] else str(i) for i in chunk.morphs])\n",
    "    else:\n",
    "        return str(chunk)\n",
    "\n",
    "\n",
    "def dst(chunk, sentence, nX, nY, end):\n",
    "    dest = int(chunk.dst)\n",
    "    if dest == -1 or int(sentence[dest].dst) == end:\n",
    "        return stri(chunk, nX, nY)\n",
    "    else:\n",
    "        return stri(chunk, nX, nY) + ' -> ' + dst(sentence[dest], sentence, nX, nY, end)\n",
    "\n",
    "def ancestor(a, b, sentence):\n",
    "    destA = int(a.dst)\n",
    "    if destA == -1:\n",
    "        return False\n",
    "    return sentence[destA] == b or ancestor(sentence[destA], b, sentence)\n",
    "\n",
    "def cousin(a, b, sentence):\n",
    "    destA = int(a.dst)\n",
    "    if ancestor(sentence[destA], b, sentence) or destA == -1:\n",
    "        return destA\n",
    "    else:\n",
    "        return cousin(sentence[destA], b, sentence) \n",
    "    \n",
    "def xy(nX, nY, sentence):\n",
    "    destX = int(nX.dst)\n",
    "    destY = int(nY.dst)\n",
    "    if ancestor(nX, nY, sentence):\n",
    "        return dst(nX, sentence, nX, nY, sentence[destY]) + '\\n'\n",
    "    else:\n",
    "        c = cousin(nX, nY, sentence)\n",
    "        return  dst(nX, sentence, nX, nY, c)+'|' + dst(nY, sentence, nX, nY, c) + '|' +dst(sentence[c], sentence, nX, nY, sentence[-1]) + '\\n'\n",
    "    \n",
    "\n",
    "with open('data/file49', 'a') as a:\n",
    "    for sentence in sentences:\n",
    "        nouns = []\n",
    "        for chunk in sentence:\n",
    "            if [i for i in chunk.morphs if i.pos == '名詞'] != []:\n",
    "                nouns.append(chunk)\n",
    "        if len(nouns) > 1:\n",
    "            for i in range(len(nouns)-1):\n",
    "                for j in range(i+1,len(nouns)):\n",
    "                    nX = nouns[i]\n",
    "                    nY = nouns[j]\n",
    "                    a.write(xy(nX, nY, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n",
      "[1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "a=[[1,2],[3,4,5],[6,7]]\n",
    "print([i for j in a for i in j])#クレイジー\n",
    "print(sum(a, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
