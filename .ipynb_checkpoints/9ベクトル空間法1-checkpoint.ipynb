{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enwiki-20150112-400-r10-105752.txt.bz2は，2015年1月12日時点の英語のWikipedia記事のうち，約400語以上で構成される記事の中から，ランダムに1/10サンプリングした105,752記事のテキストをbzip2形式で圧縮したものである．このテキストをコーパスとして，単語の意味を表すベクトル（分散表現）を学習したい．第9章の前半では，コーパスから作成した単語文脈共起行列に主成分分析を適用し，単語ベクトルを学習する過程を，いくつかの処理に分けて実装する．第9章の後半では，学習で得られた単語ベクトル（300次元）を用い，単語の類似度計算やアナロジー（類推）を行う．\n",
    "\n",
    "なお，問題83を素直に実装すると，大量（約7GB）の主記憶が必要になる． メモリが不足する場合は，処理を工夫するか，1/100サンプリングのコーパスenwiki-20150112-400-r100-10576.txt.bz2を用いよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80. コーパスの整形\n",
    "文を単語列に変換する最も単純な方法は，空白文字で単語に区切ることである． ただ，この方法では文末のピリオドや括弧などの記号が単語に含まれてしまう． そこで，コーパスの各行のテキストを空白文字でトークンのリストに分割した後，各トークンに以下の処理を施し，単語から記号を除去せよ．\n",
    "\n",
    " - トークンの先頭と末尾に出現する次の文字を削除: `.,!?;:()[]'\"`\n",
    " - 空文字列となったトークンは削除\n",
    " \n",
    "以上の処理を適用した後，トークンをスペースで連結してファイルに保存せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/large_files/file80', 'w'):\n",
    "    pass\n",
    "with open('data/large_files/file80', 'a')as a:\n",
    "    for line in bz2.open('data/large_files/enwiki-20150112-400-r10-105752.txt.bz2'):\n",
    "        lst = [word.strip('.,!?;:()[]\\'\\\"') for word in line.decode().split() if word.strip('.,!?;:()[]\\'\\\"') != '' ]\n",
    "        if lst != []:\n",
    "            a.write(' '.join(lst) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\n",
      "\n",
      "Anarchism is a political philosophy that advocates stateless societies often defined as self-governed voluntary institutions but that several authors have defined as more specific institutions based on non-hierarchical free associations Anarchism holds the state to be undesirable unnecessary or harmful While anti-statism is central anarchism entails opposing authority or hierarchical organisation in the conduct of human relations including but not limited to the state system\n",
      "\n",
      "As a subtle and anti-dogmatic philosophy anarchism draws on many currents of thought and strategy Anarchism does not offer a fixed body of doctrine from a single particular world view instead fluxing and flowing as a philosophy There are many types and traditions of anarchism not all of which are mutually exclusive Anarchist schools of thought can differ fundamentally supporting anything from extreme individualism to complete collectivism Strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications Anarchism is usually considered a radical left-wing ideology and much of anarchist economics and anarchist legal philosophy reflect anti-authoritarian interpretations of communism collectivism syndicalism mutualism or participatory economics\n",
      "\n",
      "The central tendency of anarchism as a social movement has been represented by anarcho-communism and anarcho-syndicalism with individualist anarchism being primarily a literary phenomenon which nevertheless did have an impact on the bigger currents and individualists have also participated in large anarchist organisations Many anarchists oppose all forms of aggression supporting self-defense or non-violence anarcho-pacifism while others have supported the use of some coercive measures including violent revolution and propaganda of the deed as means to achieve anarchist ends\n",
      "\n",
      "Etymology and terminology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line, _ in zip(open('data/large_files/file80'), range(5)):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 81. 複合語からなる国名への対処\n",
    "英語では，複数の語の連接が意味を成すことがある．例えば，アメリカ合衆国は\"United States\"，イギリスは\"United Kingdom\"と表現されるが，\"United\"や\"States\"，\"Kingdom\"という単語だけでは，指し示している概念・実体が曖昧である．そこで，コーパス中に含まれる複合語を認識し，複合語を1語として扱うことで，複合語の意味を推定したい．しかしながら，複合語を正確に認定するのは大変むずかしいので，ここでは複合語からなる国名を認定したい．\n",
    "\n",
    "インターネット上から国名リストを各自で入手し，80のコーパス中に出現する複合語の国名に関して，スペースをアンダーバーに置換せよ．例えば，\"United States\"は\"United_States\"，\"Isle of Man\"は\"Isle_of_Man\"になるはずである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "countries = list(pycountry.countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(i):\n",
    "    try:\n",
    "        i.official_name\n",
    "        return False\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "officials = [i.official_name for i in countries if not error(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_names = [i.name for i in countries if len(i.name.split()) > 1] + [i for i in officials if len(i.split()) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/large_files/file81', 'w'):\n",
    "    pass\n",
    "with open('data/large_files/file81', 'a')as a:\n",
    "    for line in open('data/large_files/file80'):\n",
    "        for name in long_names:\n",
    "            if name in line:\n",
    "                line = line.replace(name, '_'.join(name.split()))\n",
    "        a.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beltane is the Gaelic May Day festival Most commonly it is held on 30 April but sometimes on 1 May or about halfway between the spring equinox and the summer solstice Historically it was widely observed throughout Ireland Scotland and the Isle_of_Man In Irish it is Bealtaine in Scottish Gaelic Bealltainn and in Manx Gaelic Boaltinn or Boaldyn It is one of the four Gaelic seasonal festivals—along with Samhain Imbolc and Lughnasadh—and is similar to the Welsh Calan Mai\n",
      "\n",
      "Bonfires continued to be a key part of the festival in the modern era and were generally lit on mountains and hills Ronald Hutton writes that To increase the potency of the holy flames in Britain at least they were often kindled by the most primitive of all means of friction between wood In the 19th century for example John Ramsay described Scottish Highlanders kindling a need-fire or force-fire at Beltane Such a fire was deemed sacred In the 19th century the ritual of driving cattle between two fires—as described in Sanas Cormaic almost 1000 years before—was still practised across most of Ireland and in parts of Scotland Sometimes the cattle would be driven around a bonfire or be made to leap over flames or embers The people themselves would do likewise In the Isle_of_Man people ensured that the smoke blew over them and their cattle On Beltane Eve all hearth fires and candles would be doused and at the end of the festival they would be re-lit from the Beltane bonfire When the bonfire had died down its ashes were thrown among the sprouting crops From these rituals it is clear that the fire was seen as having protective powers Similar rituals were part of May Day Midsummer or Easter customs in other parts of the British Isles and mainland Europe According to Frazer the fire rituals are a kind of imitative or sympathetic magic According to one theory they were meant to mimic the Sun and to ensure a needful supply of sunshine for men animals and plants According to another they were meant to symbolically burn up and destroy all harmful influences\n",
      "\n",
      "The American Red Cross policy is as follows During the period January 1 1980 to December 31 1996 spending a total time of three months or more in the Channel Islands England the Falkland Islands the Isle_of_Man Gibraltar Northern Ireland Scotland and Wales precludes individuals from donating Moreover spending a total time of five years or more after January 1 1980 to present in the above-mentioned countries and/or any country in Europe except the former USSR also precludes donation People with a biologic relative having been diagnosed with CJD or vCJD are unable to donate Biologic relative in this setting means mother father sibling grandparent aunt uncle or child\n",
      "\n",
      "In New_Zealand the New_Zealand Blood Service NZBS in 2000 introduced measures to preclude permanently donors having resided in the United_Kingdom including the Isle_of_Man and the Channel Islands for a total of six months or more between January 1980 and December 1996 The measure resulted in ten percent of New_Zealand's active blood donors at the time becoming ineligible to donate blood In 2003 the NZBS further extended restrictions to preclude permanently donors having had received a blood transfusion in the United_Kingdom since January 1980 and in April 2006 restrictions were further extended to include the Republic of Ireland and France\n",
      "\n",
      "In addition to England the jurisdiction of the Church of England extends to the Isle_of_Man the Channel Islands and a few parishes in Flintshire Monmouthshire and Radnorshire in Wales Expatriate congregations on the continent of Europe have become the Diocese of Gibraltar in Europe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for line in open('data/large_files/file81'):\n",
    "    if 'Isle_of_Man' in line:\n",
    "        print(line)\n",
    "        i=i+1\n",
    "    if i==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 82. 文脈の抽出\n",
    "81で作成したコーパス中に出現するすべての単語tに関して，単語tと文脈語cのペアをタブ区切り形式ですべて書き出せ．ただし，文脈語の定義は次の通りとする．\n",
    "\n",
    "- ある単語tの前後d単語を文脈語cとして抽出する（ただし，文脈語に単語tそのものは含まない）\n",
    "- 単語tを選ぶ度に，文脈幅dは{1,2,3,4,5}の範囲でランダムに決める．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/large_files/file82', 'w'):\n",
    "    pass\n",
    "with open('data/large_files/file82', 'a')as a:\n",
    "    for line in open('data/large_files/file81'):\n",
    "        data = line.split()\n",
    "        for i, t in enumerate(data):\n",
    "            d = random.randint(1,5)\n",
    "            start = i - d if not i < d else 0\n",
    "            end = i + d + 1\n",
    "            for c in data[start: i] + data[i+1: end]:\n",
    "                a.write(t + '\\t' + c + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 83. 単語／文脈の頻度の計測\n",
    "82の出力を利用し，以下の出現分布，および定数を求めよ．\n",
    "\n",
    "- f(t,c): 単語tと文脈語cの共起回数\n",
    "- f(t,∗): 単語tの出現回数\n",
    "- f(∗,c): 文脈語cの出現回数\n",
    "- N: 単語と文脈語のペアの総出現回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_c = {}\n",
    "for line in open('data/large_files/file82'):\n",
    "    c = line.split()[1]\n",
    "    f_c.update({c: f_c.get(c, 0) + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/large_files/f_c', 'w')as w:\n",
    "    w.write(json.dumps(f_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t = {}\n",
    "for line in open('data/large_files/file82'):\n",
    "    t = line.split()[0]\n",
    "    f_t.update({t: f_t.get(t, 0) + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/large_files/f_t', 'w')as w:\n",
    "    w.write(json.dumps(f_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, line in enumerate(open('data/large_files/file82')):\n",
    "    pass\n",
    "N = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689484752"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(t,c)はでかすぎてメモリで管理できない。\n",
    "\n",
    "そこで`sort file82 | uniq -c > file83`として、file83をf(t,c)のかわりにしたいが、file82はでかすぎたので、\n",
    "1. `split -l1100000 data/large_files/file82 data/large_files/sort/`\n",
    "2. `for i in data/large_files/sort/*; do sort $i > ${i/data\\/large_files\\/sort\\//data\\/large_files\\/sorted\\/};done`\n",
    "3. `sort -m data/large_files/sorted/* | uniq -c | awk '$1 >= 10' > data/large_files/file83 `\n",
    "\n",
    "でマージソートする。（ついでにf(t,c)が10未満のものはもう使わないのでfile83には書かない。）\n",
    "\n",
    "file82とsort/とsorted/はもういらないと思うので削除する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 84. 単語文脈行列の作成\n",
    "83の出力を利用し，単語文脈行列$X$を作成せよ．\n",
    "\n",
    "ただし，行列$X$の各要素$X_{tc}$は次のように定義する．\n",
    "\n",
    "- $f(t,c)≥10$ならば，$X_{tc}={\\rm PPMI}(t,c)=max\\{\\log{\\frac{N×f(t,c)}{f(t,∗)×f(∗,c)}},0\\}$\n",
    "- $f(t,c)<10$ならば，$X_{tc}=0$\n",
    "\n",
    "ここで，PPMI(t,c)\n",
    "はPositive Pointwise Mutual Information（正の相互情報量）と呼ばれる統計量である．なお，行列X\n",
    "の行数・列数は数百万オーダとなり，行列のすべての要素を主記憶上に載せることは無理なので注意すること．幸い，行列X\n",
    "のほとんどの要素は0\n",
    "になるので，非0\n",
    "の要素だけを書き出せばよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/large_files/f_t')as r:\n",
    "    f_t = json.loads(r.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/large_files/f_c')as r:\n",
    "    f_c = json.loads(r.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689484753"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = sum([f_t[i] for i in f_t])\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/large_files/file84', 'w'):\n",
    "    pass\n",
    "with open('data/large_files/file83')as r, open('data/large_files/file84', 'a')as a:\n",
    "    for line in r:\n",
    "        data = line.split()\n",
    "        f_tc, t, c = int(data[0]), data[1], data[2]\n",
    "        tmp = (N * f_tc) / (f_t[t] * f_c[c])\n",
    "        if tmp > 1:\n",
    "            a.write(t + ' ' + c + ' ' + str(math.log(tmp)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85. 主成分分析による次元圧縮\n",
    "84で得られた単語文脈行列に対して，主成分分析を適用し，単語の意味ベクトルを300次元に圧縮せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.sparse.linalg import svds as svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138483\n"
     ]
    }
   ],
   "source": [
    "ts=[]\n",
    "tmp=''\n",
    "for line in open('data/large_files/file84'):\n",
    "    t, _, _ = line.split()\n",
    "    if t != tmp:\n",
    "        ts.append(t)\n",
    "        tmp = t\n",
    "print(len(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('data/large_files/c', 'w'):\n",
    "    pass\n",
    "with open('data/large_files/c', 'a')as a, open('data/large_files/file84')as r:\n",
    "    for line in r:\n",
    "        _, c, _ = line.split()\n",
    "        a.write(c + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bashで`sort data/large_files/c | uniq > data/large_files/c2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs=[]\n",
    "for line  in open('data/large_files/c2'):\n",
    "    cs.append(line[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_to_number = {}\n",
    "for i, t in enumerate(ts):\n",
    "    t_to_number.update({t : i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_to_number = {}\n",
    "for i, c in enumerate(cs):\n",
    "    c_to_number.update({c : i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spa.lil_matrix((len(ts),len(cs)))\n",
    "for line in open('data/large_files/file84'):\n",
    "    t, c, x = line.split()\n",
    "    t = t_to_number[t]\n",
    "    c = c_to_number[c]\n",
    "    x = float(x)\n",
    "    X[t,c] = x\n",
    "X = X.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, _ = svd(X, k=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/large_files/file85.npy', U.dot(np.diag(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 86. 単語ベクトルの表示\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"のベクトルを表示せよ．ただし，\"United States\"は内部的には\"United_States\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.load('data/large_files/file85.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "US = T[t_to_number['United_States']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 87. 単語の類似度\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"と\"U.S.\"のコサイン類似度を計算せよ．ただし，\"U.S.\"は内部的に\"U.S\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    return a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "US2 = T[t_to_number['U.S']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8423205544005085\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(US, US2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 88. 類似度の高い単語10件\n",
    "85で得た単語の意味ベクトルを読み込み，\"England\"とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "England = T[t_to_number['England']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "englands = []\n",
    "for t in t_to_number:\n",
    "    if t == 'England':\n",
    "        continue\n",
    "    cs = cos_sim(T[t_to_number[t]], England)\n",
    "    if cs > 0.5:\n",
    "        englands.append((t, cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Scotland', 0.7862958926377683),\n",
       " ('Wales', 0.7194555376212356),\n",
       " ('Australia', 0.6410951104094761),\n",
       " ('Ireland', 0.6367809246042602),\n",
       " (\"Yard's\", 0.6208629902477247),\n",
       " ('Somerset', 0.6135271692553941),\n",
       " ('Cornwall', 0.5942997079605497),\n",
       " ('Britain', 0.586502227946235),\n",
       " ('Yorkshire', 0.5811495018744646),\n",
       " ('Maugham', 0.5811093916007712)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(englands, key = lambda i: i[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 89. 加法構成性によるアナロジー\n",
    "85で得た単語の意味ベクトルを読み込み，vec(\"Spain\") - vec(\"Madrid\") + vec(\"Athens\")を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec89 = T[t_to_number['Spain']] - T[t_to_number['Madrid']] + T[t_to_number['Athens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst89 = []\n",
    "for t in t_to_number:\n",
    "    cs = cos_sim(T[t_to_number[t]], vec89)\n",
    "    if cs > 0.5:\n",
    "        lst89.append((t, cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spain', 0.9072977103255369),\n",
       " ('Portugal', 0.87639346665756),\n",
       " ('Sweden', 0.8573522072633883),\n",
       " ('Norway', 0.848648818571551),\n",
       " ('Greece', 0.8457319759782043),\n",
       " ('Denmark', 0.8413927346118384),\n",
       " ('Belgium', 0.8295460455216749),\n",
       " ('Italy', 0.8235012148294278),\n",
       " ('Britain', 0.8139039160428228),\n",
       " ('Netherlands', 0.8042173460937543)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lst89, key = lambda i: i[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スペイン(国)ーマドリード(首都)＋アテネ(別の国の首都)→国全般！　ということだろうか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
