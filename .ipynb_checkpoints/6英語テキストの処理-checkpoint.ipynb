{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50. 文区切り\n",
    "(. or ; or : or ? or !) → 空白文字 → 英大文字というパターンを文の区切りと見なし，入力された文書を1行1文の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def search(line, file):\n",
    "    x = re.search('^(?P<sente>.*?)(?P<mark>[.;:?!])( (?P<newline>[A-Z].*))?$|^(?P<title>.*)$', line)\n",
    "    if x:\n",
    "        sente = x.group('sente')\n",
    "        mark = x.group('mark')\n",
    "        newline = x.group('newline')\n",
    "        title = x.group('title')\n",
    "        if mark:\n",
    "            file.write(sente + mark + '\\n')\n",
    "        elif title:\n",
    "            file.write(title + '\\n')\n",
    "        if newline:\n",
    "            search(newline, file)\n",
    "    #else:\n",
    "        #print(line)\n",
    "    \n",
    "with open('data/file50', 'w'):\n",
    "    pass\n",
    "with open('data/file50', 'a') as a:\n",
    "    for line in open('data/nlp.txt'):\n",
    "        search(line, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 51. 単語の切り出し\n",
    "空白を単語の区切りとみなし，50の出力を入力として受け取り，1行1単語の形式で出力せよ．ただし，文の終端では空行を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(line, file):\n",
    "    x = re.search('(?P<word>[^ ]*)( (?P<newline>.*))?', line)\n",
    "    if x:\n",
    "        word = x.group('word').strip('.,;:?!()\\\"')\n",
    "        newline = x.group('newline')\n",
    "        file.write(word + '\\n')\n",
    "        if newline:\n",
    "            search(newline, file)\n",
    "        else:\n",
    "            file.write('\\n')\n",
    "\n",
    "with open('data/file51', 'w'):\n",
    "    pass\n",
    "with open('data/file51', 'a') as a:\n",
    "    for line in open('data/file50'):\n",
    "        search(line, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 52. ステミング\n",
    "51の出力を入力として受け取り，Porterのステミングアルゴリズムを適用し，単語と語幹をタブ区切り形式で出力せよ． Pythonでは，Porterのステミングアルゴリズムの実装としてstemmingモジュールを利用するとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stemming.porter2 import stem\n",
    "\n",
    "with open('data/file52', 'w'):\n",
    "    pass\n",
    "with open('data/file52', 'a') as a:\n",
    "    for line in open('data/file51'):\n",
    "        line = line.strip('\\n')\n",
    "        a.write(line + '\\t' + stem(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 53. Tokenization\n",
    "Stanford Core NLPを用い，入力テキストの解析結果をXML形式で得よ．また，このXMLファイルを読み込み，入力テキストを1行1単語の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file53', 'w'):\n",
    "    pass\n",
    "with open('data/file53', 'a') as a:\n",
    "    for line in  open('data/nlp.txt.xml'):\n",
    "        x = re.search('\\<word\\>(?P<word>.*?)\\<\\/word\\>', line)\n",
    "        if x:\n",
    "            a.write(x.group('word') + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 54. 品詞タグ付け\n",
    "Stanford Core NLPの解析結果XMLを読み込み，単語，レンマ，品詞をタブ区切り形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file54', 'w'):\n",
    "    pass\n",
    "with open('data/file54', 'a') as a:\n",
    "    for line in  open('data/nlp.txt.xml'):\n",
    "        x = re.search('\\<(?P<title>.*?)\\>(?P<info>.*?)\\<\\/(?P=title)\\>', line)\n",
    "        if x:\n",
    "            title = x.group('title')\n",
    "            info = x.group('info')\n",
    "            if title == 'word' or title == 'lemma':\n",
    "                a.write(info + '\\t')\n",
    "            elif title == 'POS':\n",
    "                a.write(info + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 55. 固有表現抽出\n",
    "入力文中の人名をすべて抜き出せ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file55', 'w'):\n",
    "    pass\n",
    "with open('data/file55', 'a') as a:\n",
    "    flag = False\n",
    "    stack = []\n",
    "    for line in  open('data/nlp.txt.xml'):\n",
    "        x = re.search('\\<(?P<title>.*?)\\>(?P<info>.*?)\\<\\/(?P=title)\\>', line)\n",
    "        if x:\n",
    "            title = x.group('title')\n",
    "            info = x.group('info')\n",
    "            if title == 'word':\n",
    "                stack.append(info)\n",
    "            elif title == 'NER' and info != 'PERSON' and not flag:\n",
    "                stack = []\n",
    "            elif title == 'NER' and info != 'PERSON' and flag:\n",
    "                a.write('\\t'.join(stack[:-1]) + '\\n')\n",
    "                flag = False\n",
    "                stack = []\n",
    "            elif title == 'NER' and info == 'PERSON':\n",
    "                flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 56. 共参照解析\n",
    "Stanford Core NLPの共参照解析の結果に基づき，文中の参照表現（mention）を代表参照表現（representative mention）に置換せよ．ただし，置換するときは，「代表参照表現（参照表現）」のように，元の参照表現が分かるように配慮せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flag1 = False\n",
    "flag2 = False\n",
    "coreference = []\n",
    "mentions = []\n",
    "mention = {}\n",
    "for line in open('data/nlp.txt.xml'):\n",
    "    x = re.search('\\<coreference\\>', line)\n",
    "    if x and not flag1:\n",
    "        flag1 = True\n",
    "        continue\n",
    "    if x and flag1:\n",
    "        flag2 = True\n",
    "        continue\n",
    "    x = re.search('\\<\\/coreference\\>', line)\n",
    "    if x and not flag2:\n",
    "        flag1 = False\n",
    "        continue\n",
    "    if x and flag2:\n",
    "        flag2 = False\n",
    "        if len(mentions) > 1:\n",
    "            coreference.append(mentions)\n",
    "        mentions = []\n",
    "        continue\n",
    "    x = re.search('\\<(?P<title>.*?)\\>(?P<info>.*?)\\<\\/(?P=title)\\>', line)\n",
    "    if x and flag2:\n",
    "        title = x.group('title')\n",
    "        info = x.group('info')\n",
    "        if title == 'text' and mentions != [] and info == mentions[0]['text']:\n",
    "            continue\n",
    "        if title == 'sentence' or title == 'start' or title =='end':\n",
    "            mention.update({title: int(info) - 1})\n",
    "        elif title == 'text':\n",
    "            mention.update({title:info})\n",
    "            mentions.append(mention)\n",
    "            mention = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreference = sorted(coreference, key = lambda mentions: len(mentions[0]['text'].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['you', 'your'],\n",
       " ['NLP', 'its'],\n",
       " ['Such models', 'they'],\n",
       " ['My head', 'your head'],\n",
       " ['machine learning algorithms',\n",
       "  'These algorithms',\n",
       "  'the earliest-used algorithms'],\n",
       " ['The machine-learning paradigm', 'The paradigm of machine learning'],\n",
       " ['hand-written rules --', 'the rules'],\n",
       " ['many speech recognition systems',\n",
       "  'These systems',\n",
       "  'these systems',\n",
       "  'the systems']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[mention['text'] for mention in mentions] for mentions in coreference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentence= []\n",
    "for line in open('data/nlp.txt.xml'):\n",
    "    x = re.search('\\<word\\>(?P<word>.*?)\\<\\/word\\>', line)\n",
    "    if x:\n",
    "        sentence.append(x.group('word'))\n",
    "    elif re.search('\\<\\/sentence\\>', line):\n",
    "        if sentence != []:\n",
    "            sentences.append(sentence)\n",
    "        sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "new_sentences = copy.deepcopy(sentences)\n",
    "for mentions in coreference:\n",
    "    for mention in mentions[1:]:\n",
    "        old_text = new_sentences[mention['sentence']][mention['start'] : mention['end']]\n",
    "        if len(old_text) == 1:\n",
    "            new_text = [mentions[0]['text'] + '(' + old_text[0] + ')']\n",
    "        else:\n",
    "            new_text = [mentions[0]['text'] + '(' + old_text[0]] + old_text[1:-1] + [old_text[-1] + ')']\n",
    "        new_sentences[mention['sentence']][mention['start'] : mention['end']] = new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['say', 'My head(you(your)', 'head)', 'hurts', '?', \"''\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentences[11][34:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['say', 'your', 'head', 'hurts', '?', \"''\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[11][34:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/file56', 'w'):\n",
    "    pass\n",
    "with open('data/file56', 'a') as a:\n",
    "    for sentence in new_sentences:\n",
    "        a.write(' '.join(sentence).replace('-LRB- ','(').replace(' -RRB-',')') + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 57. 係り受け解析\n",
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "governor = ''\n",
    "dependent = ''\n",
    "dependencies = []\n",
    "dep = []\n",
    "dic = {}\n",
    "sentences = []\n",
    "sentence = {}\n",
    "flag = False\n",
    "for line in open('data/nlp.txt.xml'):\n",
    "    if re.search('\\<dependencies type=\\\"collapsed-dependencies\\\"\\>', line):\n",
    "        flag = True\n",
    "    if re.search('\\<\\/dependencies\\>', line):\n",
    "        flag = False\n",
    "        if dep != []:\n",
    "            dependencies.append(dep)\n",
    "            sentences.append(sentence)\n",
    "        dep = []\n",
    "        sentence = {}\n",
    "    if flag:\n",
    "        x = re.search('\\<dep type=\\\"(?P<type>[^\"]*)\\\"\\>', line)\n",
    "        if x:\n",
    "            dic.update({'type': x.group('type')})\n",
    "        x = re.search('\\<governor idx=\\\"(?P<idx>[^\"]*)\\\"\\>(?P<governor>.*)\\<\\/governor\\>', line)\n",
    "        if x:\n",
    "            idx_g = int(x.group('idx'))\n",
    "            governor = x.group('governor')\n",
    "            sentence.update({idx_g: governor})\n",
    "            dic.update({'governor': idx_g})\n",
    "        x = re.search('\\<dependent idx=\\\"(?P<idx>[^\"]*)\\\"\\>(?P<dependent>.*)\\<\\/dependent\\>', line)\n",
    "        if x:\n",
    "            idx_d = int(x.group('idx'))\n",
    "            dependent = x.group('dependent')\n",
    "            sentence.update({idx_d: dependent})\n",
    "            dic.update({'dependent': idx_d})\n",
    "            dep.append(dic)\n",
    "            dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 4\n",
    "s = sentences[position]\n",
    "d = dependencies[position]\n",
    "G = nx.DiGraph()\n",
    "for i in range(len(s)):\n",
    "    if s[i] != 'ROOT':\n",
    "        G.add_node(i, label=s[i])\n",
    "for dependency in d[1:]:\n",
    "    G.add_edge(dependency['governor'], dependency['dependent'])\n",
    "g = nx.nx_agraph.to_agraph(G)\n",
    "g.draw(f\"data/file57.pdf\",format='pdf', prog='dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 58. タプルの抽出\n",
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）に基づき，「主語(subject) 述語(predicate) 目的語(object)」の組をタブ区切り形式で出力せよ．ただし，主語，述語，目的語の定義は以下を参考にせよ．\n",
    "\n",
    "- 述語: nsubj関係とdobj関係の子（dependant）を持つ単語\n",
    "- 主語: 述語からnsubj関係にある子（dependent）\n",
    "- 目的語: 述語からdobj関係にある子（dependent）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = []\n",
    "with open('data/file58', 'w'):\n",
    "    pass\n",
    "with open('data/file58', 'a')as a:\n",
    "    for i in range(len(dependencies)):\n",
    "        for dep in dependencies[i]:\n",
    "            if dep['type'] == 'nsubj':\n",
    "                pred = dep['governor']\n",
    "                sub = dep['dependent']\n",
    "                stem_pred = [stem for stem in stems if stem['pred'] == pred]\n",
    "                if stem_pred == []:\n",
    "                    stems.append({'pred' : pred, 'sub': [sub], 'obj':[]})\n",
    "                else:\n",
    "                    stem_pred[0]['sub'].append(sub)\n",
    "            if dep['type'] == 'dobj':\n",
    "                pred = dep['governor']\n",
    "                obj = dep['dependent']\n",
    "                stem_pred = [stem for stem in stems if stem['pred'] == pred]\n",
    "                if stem_pred == []:\n",
    "                    stems.append({'pred' : pred, 'sub': [], 'obj':[obj]})\n",
    "                else:\n",
    "                    stem_pred[0]['obj'].append(obj)\n",
    "        s = sentences[i]\n",
    "        for stem in stems:\n",
    "            a.write('+'.join([s[i] for i in stem['sub']]) + '\\t' + s[stem['pred']] + '\\t' + '+'.join([s[i] for i in stem['obj']]) + '\\n')\n",
    "        stems =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 59. S式の解析\n",
    "Stanford Core NLPの句構造解析の結果（S式）を読み込み，文中のすべての名詞句（NP）を表示せよ．入れ子になっている名詞句もすべて表示すること．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
